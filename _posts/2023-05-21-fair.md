---
title: 'Fairness in GNN'
date: 2023-05-21
permalink: /posts/2023/05/PMLP/
tags:
  - fairness
  - GNN
  - reflection
---
**Fairness in GNN**

之前干活的时候在这个小领域翻阅了很久，现在简单回忆并梳理一下自己对此的理解。

“公平性”这个概念初看上去会令人疑惑：怎么样才算是公平？但其实这个概念可能比想象中的要更well-defined一些。至少已经有两个相对比较成熟可以量化的概念了。

**公平性的定义**

**DP**

*Demographic Parity (a.k.a. Statistical Parity and Independence) is first introduced as a notion of group fairness based on binary sensitive feature(s) in binary classification tasks [44]. The binary sensitive feature divides the population into two sensitive subgroups (e.g., male/female). In binary classification tasks such as deciding whether a student should be admitted into a university or not, demographic parity is considered as achieved if the model yields the same acceptance rate for individuals in both sensitive subgroups. In graph mining, we first introduce demographic parity in node classification, followed by several extensions.*

*Demographic Parity in Node Classification. In node classification, we assume Y , S ˆ ∈ {0, 1} are the random variables representing the predicted class label and sensitive feature of a random node in the input graph, respectively. The criterion of demographic parity is then formulated as*

$$P(\hat{Y} = 1|S = 0) = P(\hat{Y} = 1|S = 1)$$

*To quantify to what extent the demographic parity is satisfied, ∆DP is defined when both the predicted labels and sensitive feature(s) are binary [39], [96]. The formulation of ∆DP is given as*

$$∆_{DP} = |P(\hat{Y} = 1 | S = 0) − P(\hat{Y} = 1 | S = 1)|$$

省流：二分类场景下，我们假设关注的特征S也只有两种取值，那么特征S为0还是1不影响最后被分类为正的概率，那么就是符合DP的，两者的概率差值越低越“公平”。

**EO**

*Equality of Opportunity extends the notion of equality of odds [65]. Specifically, in binary classification tasks, equality of opportunity only requires the positive predictions to be independent of sensitive feature(s) for individuals with positive ground truth labels [65]. We introduce equality of opportunity in node classification tasks as follows.*

*Equality of Opportunity in Node Classification. The criterion of equality of opportunity is given as*

$$P(\hat{Y} = 1|S = 0, Y = 1) = P(\hat{Y} = 1|S = 1, Y = 1)$$

*It should be noted that in most cases, Yˆ = 1 is an advantaged prediction [65]. Therefore, the intuition of equality of opportunity can be interpreted as: we want to avoid assigning disadvantaged predictions to individuals qualified for advantaged ones only because of their sensitive subgroup membership. In this regard, equality of opportunity is often advocated for economic justice [12], and a typical application scenario is job candidate selection. We then introduce a commonly employed quantitative metric ∆EO for equality of opportunity in node classification. Specifically, ∆EO measures how far the prediction deviates from the ideal situation that satisfies equality of opportunity. ∆EO is formally given as*

$$∆_{EO} = |P(\hat{Y} = 1 | Y = 1, S = 0) − P(\hat{Y} = 1 | Y = 1, S = 1)|$$

省流：二分类场景下，同样假设关注的特征S也只有两种取值，那么**在分类真值确实为1**时，特征S为0还是1不影响最后预测值分类为正的概率，那么就是符合EO的，两者的概率差值越低越“公平”。

这两个指标之间的差别不算特别大，不过要点在于它们比较容易算出来，于是很多时候就变成了改模型然后在这两个指标上刷榜。

事实上也有很多文章试图构造出更多的指标以更好地去表现“公平”，但很多时候出于比较性能的需要还是会回到这两个指标上来。

**Fairness in GNN**

这个要点被引入GNN的理由据称是：由于GNN的消息传播机制不仅考虑节点自己的特征本身，更要考虑邻居的性质，那么节点所属子图的local拓扑结构会导致消息传播以后“不公平”的程度甚至会被加重。结果就是GNN的unfairness甚至比普通MLP（假设MLP只需要考虑节点本身特征的敏感性）要多考虑一层（“结构加深了bias”或“sensitive information leaked”）。

所以在GNN中增强这种fairness的手段也自然还是着眼于传播过程，常见的方式包括

-   为消息传播中的loss加上一个构造好的惩罚项，其理论上会尽可能减小sensitive feature在分类中的作用
-   因为消息传播（图的拓扑结构）被认为有重大影响，故可以在拓扑结构上下功夫
    -   人为加边：让其他异质（本文异质均指敏感特征值不同）节点的信息补充或者中和过来，避免形成过于封闭的局部子图
    -   人为删边或者做mask：类似，比如在FairV中强调有些边在传播后会明显加深分类与敏感特征的关联程度，删边起到一定的去相关作用
-   印象里还有一些对图做了更大的调整

同时注意到，这些措施意味着往往这些“提高公平性”的措施会多少伤害到原始分类任务的准确率，把所有节点无脑全分成1当然也很“公平”，公平性指标与准确率指标会形成一个trade off问题。据称一些文献中提出的方法在实际落地中引起了“collapse”，有人将其归因为落地造成的distribution shift中，原来学到的参数考量在实际新的distribution里失效。我对此持略怀疑的态度。

尽管有不少人围绕着“减小/衡量敏感特征影响”下功夫（比如FMP），但直觉上单纯地将敏感特征剥离出来并不能完全解决这个问题。你想要的不仅是敏感特征这么个布尔值“不起作用”，某种程度上你想要的是敏感特征不仅起作用而且起的是“抑制性”的作用。

举个例子，设想你设计了一个MLP进行分类任务，假设场景是社交网络，然后你希望分类尽可能不要以族裔为依据，单纯去掉族裔特征是不能解决这种问题的。即使模型拿不到族裔信息，也完全可以设想最终MLP参数里的某一层就是捕捉其他相关的cofounder（比如你爱吃西瓜炸鸡还是牛油果），然后在这一层用这些信息自行进行族裔分类，再把分类结果送给下一层为主要分类任务提供依据，最后事实上还是用上了敏感特征。只有模型能够主动地考量到敏感信息的可能影响并有意识地“抵制”它，你才能实现最终结果的去敏感特征化，这部分来说跟人类自己的行为方式是一致的。

实际跑代码的过程中发现，fairness in GNN研究中的常见数据集（credit、german等）很多时候是有问题的：敏感特征很多时候是人为定义的，判断依据是人的独断。这意味着这些敏感特征在数据集中的比例**很可能是不均匀的**，原始任务的要分类label的比例**也不是均匀的**，但人们通常不考虑这一点。

于是又回到之前那个例子，全部预测为1，公平性指标非常好，你的F1和auc这种指标甚至还不低。

**Future of fairness notion**

不少人听到这个概念便开始怀疑“这个东西究竟是不是个真问题”。说实话也确实算是有理的：这个东西本质上涉及用“大人们的事情（政治）”去人为地限制模型的表示能力。就拿分类问题来说，也许你想要避免涉及到的特征确实就是分类的重要依据，去掉它的影响就是会伤害性能，这时候硬做就有那么点掩耳盗铃的意味。不过实际研究中，提高公平性指标可以做到对准确率伤害很小甚至反而提高准确率。

但掩耳盗铃不也正是成年人生活难以缺席的一环么（笑）。

但回顾处理这个问题的手段，笔者个人把他理解为令模型对单个任意指定的特征做去相关化，而去相关、因果之流说实话也是近年来非常常见的热点。在去相关的研究范畴中，模型可能会学到一些错误的伪相关知识，通过进一步的设计，可以使得模型摆脱浅层的伪相关关系（比如图像分类猫狗不该与背景的天气情况有关），在更深层的相关关系中建立正确的表示。笔者觉得这两个研究之间是明显有联系的，在后者中错误的伪相关可能不确定，在前者中我们算是显式地指定了要“去相关”的因素。

在这个基础上，如果人类显式指定的要求不限于“公平”，而包括比如某些要求很强的domain knowledge，那么笔者觉得是有可能帮助模型学得更好、更深入的。

更进一步，笔者觉得这种显式指定去相关的机制，与其说是一个“问题”，不如更多地说是一种“需求”。如果AI模型想要更好地落地，起便利人生活的作用，那么早晚要在实现中面临各种非常dirty的需求。这种时候也许这些研究是能够派上用场、解决问题的，但这意味着不能死磕“公平”这个形而上的理念太深。
