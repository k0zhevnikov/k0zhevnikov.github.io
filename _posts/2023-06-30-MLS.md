---
title: '阅读笔记-ML Security'
date: 2023-06-29
permalink: /posts/2023/06/MLS/
tags:
  - reading note
  - paper
  - Security
---
**阅读笔记**

**A Review of Adversarial Attack and Defense for Classification Methods**

**笔者评价**

在这篇综述之前，笔者接触到的该领域相关介绍就是李宏毅老师在其ML2021课程中的内容（https://youtu.be/xGQKhbjrFRk）。两个材料都是2021年的，约等于是同一时期的工作。

与上述视频相比，此文更多着眼于问题的形式化，给出了常见思路的公式化与对应的文献，没有涉及一些比较奇技淫巧或者玩的部分（比如所谓one pixel attack，general attack，训练样本里加后门等），但对具体的攻防机制有更多更细致的介绍，以及通过实验能更好地量化一般攻防具体能够达到的成功率。这些材料给我的印象是：至2021年，ML security这个领域的很多工作都是探索性的，试图看看“能做到什么程度”。

但这个领域似乎明显是能与传统安全/统计联系起来的：典型例子如（伪）随机数生成器似乎是安全实战中的常见攻击对象，其也可以视为一个黑盒，通过攻击者不知道的，对标准的概率分布进行某种不完美的拟合产生结果。因为拟合必然不是完美的，故存在伪随机数黑盒本身特有而原分布不存在的一些特征。通过捕捉到这些独有特征，攻击者就可以理解到生成器内部的信息，进而专门针对伪随机数生成器发起攻击。

这个例子完全可以迁移过来：**神经网络这个黑盒试图识别某些特定模式，学习某种表征（高维向量），也因为表征本身不会完美，分类的依据总会带有神经网络模型特有的残余噪声，抓到这个噪声特征就能明显地干扰其正常的分类模式**。

那么似乎可以认为：在类似领域的工具和理论可能已经比较成熟，且应该是能迁移过来，乃至落地到实际上线项目中的，但总是需要人做。

我更好奇的是：考虑到AIGC的热门与高产出，这种基于有监督训练与分类任务的安全研究能否拓展到生成任务中？能否基于此发展出更多手段，比如说，让AI生成一些常人难以发觉的有害内容？或者借助AI生成的内容实现某种加密通信？李宏毅老师的视频里点到了一部分，感觉还有很多有趣的思路可以尝试。

但考虑到现在这些工作的攻击方式很多还是基于梯度或者拟合梯度带来的信息来捕捉高维独有特征，迁移到生成任务感觉并不容易。

**Abstract**

*many classification methods, especially deep neural networks (DNNs), are vulnerable to adversarial examples; i.e.,* **examples that are carefully crafted** *to fool a well-trained classification model while being indistinguishable from natural data to human.*

**1 Introduction**

*By adding* **imperceptible perturbations** *to the original inputs, the attacker can produce adversarial examples to fool a learned classifier (e.g., Szegedy et al., 2014; Goodfellow et al., 2015). Adversarial examples are* **indistinguishable** *from the original inputs* **to human***, but are mis-classified by the classifier*

*...*

*an adversarial example is not an isolated case that happens only to some neural networks, but a general case for most of the practically used neural networks*

且非神经网络的模型也被验证到易受攻击

研究方式接近传统安全，两种方式：攻与防

**3 Attack**

*Adversarial attacks are processes to generate an adversarial example based on a given natural sample and the victim model. ... An adversarial attack aims to find a small perturbation δ such that the adversarial example x ∗ = x0 + δ, which looks similar to x0 to humans, will be mis-classified by the victim model.*

通过给输入加上某个人无法识别的扰动从而干扰输出的结果。

两种goals

-   targeted，要求更高
-   untargeted

three categories:

-   gradient-based,
-   score-based,
-   decision-based.

三种方式可以复合，可能实现更强的攻击。

几种主要的scenario

-   白盒：受攻击模型所有的信息（模型结构、参数）已知
-   软标记黑盒：能够知道分类得到的得分
-   硬标记黑盒：只能知道分类到的标签
-   灰盒：有多种设计

**3.1 Gradient-Based Attack**

利用梯度值生成攻击样本，也因为需要获得梯度值所以用于白盒场景。生成对抗性示例的过程可以被形式化为一个优化问题。根据攻击方法的优化形式，基于梯度的攻击方法可以进一步分为两个子类别。

*For instance, the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) generates adversarial examples based on the sign of gradients, and uses a step size to control the l∞ norm of perturbation. The Basic Iterative Method (BIM) (Kurakin et al., 2016) and Projected Gradient Descent Attack (PGD) (Madry et al., 2018) can be viewed as iterative versions of FGSM. PGD crafts adversarial examples by running FGSM for a fixed number of iterations or until mis-classification is achieved. The most effective gradient-based adversarial attack methods to date are C&W (Carlini and Wagner, 2017b) and PGD. Both C&W attack and PGD have been frequently used to benchmark the defense algorithms due to their effectiveness (Athalye et al., 2018).*

-   Constraint-optimization formulation based methods
-   Regularization-optimization formulation based methods

**基于约束优化形式的方法**

为发起攻击形式化了一个优化问题，其目标是寻找导致错误分类的对抗性扰动。

-   对于非目标攻击，损失函数L可以是用于训练分类器的损失函数
-   而对于目标攻击，则应使用**设计用于目标类别**的其他损失函数。

通过最大化最优化得到的式子，攻击者迫使分类器在分类任务上出错。为了确保扰动不会太大，以至于x0和x∗在人眼中无法区分，搜索空间被限制在输入周围的ε-球内。

这个被构造出的优化式子可以用梯度下降法求解

**投影梯度下降攻击（PGD）**通过使用投影梯度下降方法解决这个优化问题来生成对抗性示例，可以在给定的ε-球中找到对抗性示例。PGD攻击沿着减小原始类别概率最大的方向进行更新，然后将结果投影回输入的ε-球中。

*Basically, PGD attack constructs an adversarial example by adding or subtracting a small error term, α, to each input dimension. The decision of adding or subtracting an error term depends on whether the sign of the gradient for an input dimension is positive or negative. Then the result from previous step is projected to the -ball around the original input. If (3) is run for only one step, it is equivalent to the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015). If it is run for multiple steps, it is the PGD attack or the Basic Iterative Method (BIM) (Kurakin et al., 2016). Recently, Croce and Hein (2020) introduced a new attack method called Autoattack, which combines two extensions of PGD and a decision-based attack to automatically evaluate defense methods without parameter tuning.*

**基于正则化优化形式的方法**

代表：由Carlini和Wagner（2017a）提出的**C&W**攻击，迄今为止最强大之一。它可以进行有目标和无目标攻击，其形式化为以下优化问题：  
x∗ = argmin \|\|x - x0\|\|2 + c · g(x)  
其中第一项强制原始输入的扭曲较小，**第二项是衡量攻击成功的损失函数**。参数c \> 0控制扭曲程度和攻击成功之间的权重。

-   如果是无目标攻击，意味着攻击者只希望分类器出错，不关心对抗性示例的预测标签，那么最小化g(x)将使真实类别y0的预测得分小于除y0以外的其他类别的预测得分。因此，对抗性示例x∗将被错误地分类到其他类别。
-   如果攻击者希望对抗性示例被分类为具有标签t的特定目标类别（其中t≠y0），那么有目标攻击损失函数将推动目标类别t的预测得分高于其他类别。Carlini和Wagner（2017a）证明了他们的攻击可以成功绕过针对检测对抗性示例的十种不同防御方法。

弹性网络攻击（**EAD**）将生成对抗性示例的过程形式化为一个弹性网络正则化的优化问题，可以看作是C&W攻击的扩展版本。他们对于针对带标签样本生成对抗性示例的弹性网络攻击的公式如下：**（多了一项）**  
x∗ = argmin \|\|x - x0\|\|2 + β\|\|x - x0\|\|1 + c · g(x)，  
其中g(x)与C&W攻击中定义的相同。作者表明，EAD可以提高攻击的可传递性并补充对抗性训练。

**在对抗性攻击中扰动集的选择。**

无论是约束优化还是正则化优化的形式，**都需要一种距离度量来量化原始示例和扰动示例之间的差异**。上述例子中使用了多种范数，但可以认为p范数的形式在此不太灵活，因此研究人员开始探索更灵活的扰动集。例如，Wong等人（2019）使用Wasserstein距离来衡量扰动强度，而Wong和Kolter（2020）提出了基于人类感知（？）来学习扰动集的方法。

**3.2 Score-Based Attack**

基于受攻击分类器的输出评分进行对抗性攻击。

**梯度近似方法**

首先近似梯度或梯度的符号，然后利用近似的信息生成对抗性示例。

**零阶优化攻击（ZOO）**使用有限差分方法来近似损失函数相对于输入的梯度。然后，应用C&W攻击生成对抗性示例。

**其他方法**

其他方法作为一个子类不使用梯度相关信息生成对抗性示例。

**高斯黑盒对抗攻击（Nattack）**，它通过将对抗性种群建模为高斯分布来搜索对抗性示例。这个方法的灵感来自于这样一个事实：使用不同的攻击方法可以找到相同输入的各种对抗性示例，这表明存在一个对抗性示例的**种群**。（有点意思）

**3.3 Decision-Based Attack**

在许多实际情况下，攻击者只能访问模型预测的标签，而无法获取任何梯度或得分信息。当只有预测标签 c(x) 可用时，前两种均无法使用。

-   转移攻击方法，它仅需要观察模型预测的标签。**其主要思想是训练一个与目标模型相似的替代模型，并攻击替代模型**（李宏毅提过的思路）。
-   基于决策边界的边界攻击方法，该方法通过**在决策边界上进行随机游走来搜索对抗性样本**。已经提出了许多改进边界攻击的扩展方法，以提高其效率和性能。
-   还有一些不是基于转移或随机游走的决策攻击方法。

总体而言，决策攻击的目标是生成**能够被受攻击模型返回**的对抗性样本。决策攻击方法可以分为三个子类别。

**基于转移的攻击**

**如果两个深度神经网络使用相似的数据进行训练，即使这两个模型具有非常不同的结构，一个模型上生成的对抗性样本也可以用来欺骗另一个模型。**基于这一观察，Papernot等人（2017）提出了在少量训练数据基础上训练替代模型并基于替代模型生成对抗性样本的方法。他们表明，在替代任务中生成的对抗性样本也可以欺骗目标分类器。**该方法不需要太多的样本来训练替代模型**，并且可以在非定向任务中达到相对较高的成功率。然而，**在定向任务中该方法效果不佳**。Liu等人（2017）提出了一种基于集成的方法来生成可转移的对抗性样本。这种集成方法增加了将目标对抗性样本与其目标标签转移的比例。

**基于随机游走的攻击**

第二个类别包括基于随机游走边界的方法。Brendel等人（2018）提出了边界攻击，一种不依赖于损失函数对输入的梯度并在有定向和非定向设置下表现良好的方法。该方法从一个被分类为目标类的样本开始，寻求最小化扰动同时保持对抗性。

边界攻击（Boundary Attack）在训练用于分类的深度神经网络上取得了与最先进的白盒攻击相媲美的性能。Chen和Jordan（2019）引入了Boundary Attack++，利用决策边界处的二进制信息来估计梯度方向。他们表明，Boundary Attack++所需的模型查询次数明显少于Boundary Attack。Guo等人（2020）提出了一种基于决策的黑盒攻击，通过将对对抗性样本的搜索限制在低频域中，提高了Boundary Attack的查询效率。还有许多其他方法属于这个类别，例如Guessing Smart（Brunner等人，2019）。

**基于优化的攻击**

最近的研究人员发现，与进行随机游走的过程（没有任何收敛保证）不同，决策攻击也可以被表述为解决零阶优化问题（Cheng等人，2019b）。他们表明，在决策攻击的设置中，PGD和C&W损失都是不明确的，我们需要重新定义问题，即寻找最佳方向θ的对抗性样本。给定x0，可以定义一个函数g(·)，用于测量沿着方向θ在x0和决策边界之间的距离。

虽然无法直接计算g(θ)的梯度，但**可以通过二分搜索计算g(θ)的函数值**，因此可以应用标准的零阶优化求解器来解决式(6)。

在OPT攻击（Cheng等人，2019a）中，使用摆脱随机梯度（Randomized Gradient-Free，RGF）方法（Nesterov和Spokoiny，2017）来解决这个问题。后来，Cheng等人（2020a）展示了式(6)的梯度符号可以以更高效的方式计算，从而得到了一种改进的攻击方法称为Sign-OPT。另一方面，Chen等人（2020）提出了另一种基于优化的公式，导致了一种名为HotSkipJump Attack的查询高效算法。最近，Chen和Gu（2020）提出了RayS，将Cheng等人（2019a）中的连续优化问题转化为离散问题，用于l∞范数攻击。

**4 Defense**

四个主要类别：

-   使用对抗性示例增强训练数据，
-   利用**随机性**来抵御对抗性攻击，
-   使用投影去除对抗性扰动，以及
-   检测对抗性示例而不是正确分类。

……待续

**5 Numerical Experiments**

……待续

**6 Concluding Remarks**

设想的未来研究方向：

对抗性示例的统计属性。该领域的工作还比较稀少。

改进决策性攻击的高效建模。**分类系统通常限制外部人员可以进行的查询数量**，因此许多当前的决策性攻击方法将失败。感觉像是设计一种巧妙的”探针“序列来尽可能全面地捕捉到分类器的信息？

鲁棒防御方法的理论保证。形式化地更好地正式定义鲁棒性的概念，进而实现某种量化。
